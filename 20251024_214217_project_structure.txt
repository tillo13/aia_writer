Number of files: 14
Number of directories: 6

Directory structure:
.
static
static/stories
templates
utilities
utilities/__pycache__

List of file paths by type:

CSS Files (1):
  ./static/style.css

HTML Files (3):
  ./templates/base.html
  ./templates/chat.html
  ./templates/style.html

JS Files (1):
  ./static/app.js

PY Files (5):
  ./app.py
  ./gather_files.py
  ./utilities/anthropic_utils.py
  ./utilities/google_secret_utils.py
  ./utilities/replicate_utils.py

TXT Files (4):
  ./requirements.txt
  ./static/stories/story1.txt
  ./static/stories/story2.txt
  ./static/stories/story3.txt

================================================================================
FILE CONTENTS
================================================================================

FILE: ./requirements.txt
--------------------------------------------------------------------------------
flask
anthropic
google-cloud-secret-manager
replicate

================================================================================

FILE: ./gather_files.py
--------------------------------------------------------------------------------
import os
from datetime import datetime

# Configuration
# Set file types to include
FILE_EXTENSIONS = [
    '.py',    # Python files (always included)
    '.html',  # HTML files
    '.js',    # JavaScript files
    '.css',   # CSS files
    '.json',  # JSON configuration files
    '.md',  #  configuration files
        '.txt',  #  configuration files
]

# Filename patterns to exclude (new)
EXCLUDED_FILENAME_PATTERNS = [
    'copy',   # Any file with 'copy' in the name
    'test',   # Any file with 'test' in the name
    'backup', # Any file with 'backup' in the name
    'temp',   # Any file with 'temp' in the name
]

# Directories to completely exclude from both scanning and output
EXCLUDED_DIRECTORIES = [
    ".venv", 
    ".git", 
    "venv", 
    "venv_kumori", 
    "css_OLD"
]

def gather_files(root_dir, excluded_directories, file_extensions, excluded_patterns):
    """
    Gathers files with specified extensions within the root directory and its subdirectories,
    excluding specified directories and filename patterns.

    Parameters:
        root_dir (str): The root directory to search for files.
        excluded_directories (list): List of directory names to exclude.
        file_extensions (list): List of file extensions to include.
        excluded_patterns (list): List of filename patterns to exclude.

    Returns:
        tuple: (files_data, included_directories)
    """
    files_data = []
    included_directories = set()

    for dirpath, dirnames, filenames in os.walk(root_dir):
        # Get the relative path
        relative_path = os.path.relpath(dirpath, root_dir)
        
        # Skip excluded directories - check if any part of the path matches exclusion patterns
        should_exclude = False
        for excluded_dir in excluded_directories:
            # Check both exact match and path-based matches
            if excluded_dir == relative_path or excluded_dir in relative_path.replace('\\', '/'):
                should_exclude = True
                break
                
        if should_exclude:
            continue

        # Add directory to our structure
        included_directories.add(relative_path)
        
        # Process files
        for filename in filenames:
            file_path = os.path.join(dirpath, filename)
            
            # Skip files with excluded patterns in the filename
            if any(pattern.lower() in filename.lower() for pattern in excluded_patterns):
                continue
                
            # Only include files with the specified extensions
            if any(filename.endswith(ext) for ext in file_extensions):
                try:
                    with open(file_path, 'r', encoding='utf-8') as file:
                        file_contents = file.read()
                    files_data.append((file_path, file_contents))
                except UnicodeDecodeError:
                    print(f"Could not read file {file_path} due to encoding error. Skipping.")
                except Exception as e:
                    print(f"Error with file {file_path}: {e}")

    return files_data, sorted(included_directories)

def write_to_file(output_filepath, files_data, included_directories):
    """
    Writes the gathered data to a file with project information and file contents.
    """
    with open(output_filepath, 'w', encoding='utf-8') as file:
        # Write statistics
        file.write(f"Number of files: {len(files_data)}\n")
        file.write(f"Number of directories: {len(included_directories)}\n\n")
        
        # Write directory structure
        file.write("Directory structure:\n")
        for directory in included_directories:
            file.write(f"{directory}\n")
        file.write("\n")
        
        # Group and list files by extension
        extension_groups = {}
        for filepath, _ in files_data:
            ext = os.path.splitext(filepath)[1].lower()
            if ext not in extension_groups:
                extension_groups[ext] = []
            extension_groups[ext].append(filepath)
        
        file.write("List of file paths by type:\n")
        for ext, filepaths in sorted(extension_groups.items()):
            file.write(f"\n{ext.upper()[1:]} Files ({len(filepaths)}):\n")
            for filepath in sorted(filepaths):
                file.write(f"  {filepath}\n")
        file.write("\n")
        
        # Write file contents
        file.write("="*80 + "\n")
        file.write("FILE CONTENTS\n")
        file.write("="*80 + "\n\n")
        
        for filepath, file_contents in files_data:
            file.write(f"FILE: {filepath}\n")
            file.write("-"*80 + "\n")
            file.write(f"{file_contents}\n\n")
            file.write("="*80 + "\n\n")

def scan_project_structure():
    """
    Main function to scan the project structure and write the results to a file.
    """
    root_dir = "."  # Current directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filepath = f"{timestamp}_project_structure.txt"
    
    print(f"Starting to scan project structure at {root_dir}...")
    print(f"Including file types: {', '.join(FILE_EXTENSIONS)}")
    print(f"Excluding file patterns: {', '.join(EXCLUDED_FILENAME_PATTERNS)}")
    print(f"Excluding directories: {', '.join(EXCLUDED_DIRECTORIES)}")
    
    # Gather files and directory information
    files_data, included_directories = gather_files(
        root_dir, 
        EXCLUDED_DIRECTORIES, 
        FILE_EXTENSIONS,
        EXCLUDED_FILENAME_PATTERNS
    )
    
    print(f"Found {len(files_data)} files across {len(included_directories)} directories.")
    print(f"Excluded directories won't appear in the output file.")
    
    # Write the output file
    write_to_file(
        output_filepath, 
        files_data, 
        included_directories
    )
    
    print(f"Project structure has been written to {output_filepath}")
    print(f"File size: {os.path.getsize(output_filepath) / (1024*1024):.2f} MB")

if __name__ == "__main__":
    scan_project_structure()

================================================================================

FILE: ./app.py
--------------------------------------------------------------------------------
import os,sys,subprocess,webbrowser,threading,time,logging,concurrent.futures
from flask import Flask,render_template,request,jsonify,Response
from utilities.anthropic_utils import analyze_style_and_fetch_news,restyle_content,stream_chat,fetch_news_only
from utilities.replicate_utils import generate,get_models

logging.basicConfig(level=logging.INFO,format='%(asctime)s-%(levelname)s-%(message)s')
logger=logging.getLogger(__name__)

app=Flask(__name__)
MODELS={"Sonnet 3.5 (Fast)":"claude-3-5-sonnet-latest","Sonnet 4 (Balanced)":"claude-sonnet-4-20250514","Opus 4.1 (Smartest)":"claude-opus-4-1-20250805","Haiku 3.5 (Fastest)":"claude-3-5-haiku-latest"}

# News cache - 5min TTL
NEWS_CACHE={"news":None,"ts":0}

def kill_port_5000():
    try:
        result=subprocess.run(['lsof','-ti:5000'],capture_output=True,text=True,timeout=5)
        if result.returncode==0 and result.stdout.strip():
            for pid in result.stdout.strip().split('\n'):
                try:logger.info(f"ğŸ”¥ Kill {pid}");subprocess.run(['kill','-9',pid],timeout=5)
                except Exception as e:logger.warning(f"Could not kill {pid}: {e}")
        else:logger.info("âœ… Port free")
    except:subprocess.run(['pkill','-f','flask.*5000'],timeout=5)

@app.route('/')
def home():
    logger.info("ğŸ  Home page loaded")
    return render_template('style.html')

@app.route('/chat')
def chat_page():
    logger.info("ğŸ’¬ Chat page loaded")
    return render_template('chat.html',models=MODELS)

@app.route('/prefetch_news')
def prefetch_news():
    """Pre-fetch news on file button click - cached 5min"""
    logger.info("="*80)
    logger.info("ğŸ“¡ /prefetch_news ENDPOINT CALLED")
    logger.info("â±ï¸  Request received at: %s", time.strftime('%H:%M:%S'))
    
    now=time.time()
    
    # Check cache
    if NEWS_CACHE["news"] and (now-NEWS_CACHE["ts"])<300:
        cache_age = int(now - NEWS_CACHE["ts"])
        logger.info("ğŸ’¾ RETURNING CACHED NEWS")
        logger.info(f"ğŸ“Š Cache age: {cache_age} seconds (TTL: 300s)")
        logger.info(f"ğŸ“° News length: {len(NEWS_CACHE['news'])} characters")
        logger.info("="*80)
        return jsonify({"news":NEWS_CACHE["news"],"cached":True})
    
    # Fetch fresh news
    logger.info("ğŸ”„ Cache expired or empty - fetching fresh news")
    fetch_start = time.time()
    
    try:
        logger.info("ğŸš€ Calling fetch_news_only()...")
        news=fetch_news_only()
        
        fetch_duration = time.time() - fetch_start
        NEWS_CACHE["news"]=news
        NEWS_CACHE["ts"]=now
        
        logger.info("âœ… NEWS FETCH COMPLETE")
        logger.info(f"â±ï¸  Fetch duration: {fetch_duration:.2f}s")
        logger.info(f"ğŸ“° News length: {len(news)} characters")
        logger.info(f"ğŸ’¾ Cached for 300 seconds")
        logger.info("="*80)
        
        return jsonify({"news":news,"cached":False})
    except Exception as e:
        fetch_duration = time.time() - fetch_start
        logger.error("="*80)
        logger.error("âŒ NEWS FETCH ERROR")
        logger.error(f"â±ï¸  Failed after: {fetch_duration:.2f}s")
        logger.error(f"ğŸ“› Error: {e}")
        logger.error("="*80)
        return jsonify({"error":str(e)}),500

@app.route('/analyze',methods=['POST'])
def analyze():
    """Analyze writing style from uploaded files"""
    logger.info("="*80)
    logger.info("ğŸ“„ /analyze ENDPOINT CALLED")
    
    files=request.files.getlist('files')
    if not files:
        logger.warning("âš ï¸  No files provided")
        return jsonify({'error':'No files'}),400
    if len(files)>5:
        logger.warning("âš ï¸  Too many files: %d", len(files))
        return jsonify({'error':'Max 5 files'}),400
    
    logger.info(f"ğŸ“ Received {len(files)} files:")
    for f in files:
        logger.info(f"   ğŸ“ {f.filename} ({len(f.read())/1024:.1f} KB)")
        f.seek(0)  # Reset file pointer after reading size
    
    analyze_start = time.time()
    logger.info("ğŸ”¬ Starting style analysis...")
    
    try:
        style_json,news = analyze_style_and_fetch_news(files)
        
        analyze_duration = time.time() - analyze_start
        logger.info("âœ… ANALYSIS COMPLETE")
        logger.info(f"â±ï¸  Total duration: {analyze_duration:.2f}s")
        logger.info(f"ğŸ“Š Style JSON type: {type(style_json).__name__}")
        logger.info("="*80)
        
        return jsonify({'style_json':style_json,'news':news})
    except Exception as e:
        analyze_duration = time.time() - analyze_start
        logger.error("âŒ ANALYSIS ERROR")
        logger.error(f"â±ï¸  Failed after: {analyze_duration:.2f}s")
        logger.error(f"ğŸ“› Error: {e}",exc_info=True)
        logger.error("="*80)
        return jsonify({'error':str(e)}),500

@app.route('/analyze_style',methods=['POST'])
def analyze_style():
    """Legacy endpoint - redirects to /analyze"""
    logger.info("âš ï¸  Legacy /analyze_style called, processing...")
    return analyze()

@app.route('/restyle',methods=['POST'])
def restyle():
    logger.info("="*80)
    logger.info("ğŸ¨ /restyle ENDPOINT CALLED")
    
    data=request.json
    if not data.get('style')or not data.get('news'):
        logger.warning("âš ï¸  Missing style or news data")
        return jsonify({'error':'Missing data'}),400
    
    logger.info("ğŸ¤– Using Claude Sonnet 4")
    restyle_start = time.time()
    
    try:
        styled=restyle_content(data['style'],data['news'])
        
        # Calculate cost (Claude Sonnet 4 pricing)
        # Input: $3/MTok, Output: $15/MTok (approximate)
        input_tokens = (len(str(data['style'])) + len(data['news'])) / 4  # rough estimate
        output_tokens = len(styled) / 4  # rough estimate
        cost = (input_tokens * 3 / 1_000_000) + (output_tokens * 15 / 1_000_000)
        
        restyle_duration = time.time() - restyle_start
        logger.info("âœ… RESTYLE COMPLETE")
        logger.info(f"â±ï¸  Duration: {restyle_duration:.2f}s")
        logger.info(f"ğŸ“ Output length: {len(styled)} characters")
        logger.info(f"ğŸ’° Cost: ${cost:.6f}")
        logger.info("="*80)
        
        return jsonify({'styled':styled,'cost':cost})
    except Exception as e:
        restyle_duration = time.time() - restyle_start
        logger.error("âŒ RESTYLE ERROR")
        logger.error(f"â±ï¸  Failed after: {restyle_duration:.2f}s")
        logger.error(f"ğŸ“› Error: {e}",exc_info=True)
        logger.error("="*80)
        return jsonify({'error':str(e)}),500

@app.route('/restyle_with_model',methods=['POST'])
def restyle_with_model():
    logger.info("="*80)
    logger.info("ğŸ¨ /restyle_with_model ENDPOINT CALLED")
    
    data=request.json
    if not data.get('style')or not data.get('news')or not data.get('model'):
        logger.warning("âš ï¸  Missing required data")
        return jsonify({'error':'Missing data'}),400
    
    model = data['model']
    logger.info(f"ğŸ¤– Model: {model}")
    
    prompt=f"""You are a writing style adapter. You have been given:

1. A detailed JSON style profile of a writer
2. An AI news article (originally written by {data.get('claude_version','Claude Sonnet 4')})

Your task: Rewrite the news article to match the writer's style perfectly.

STYLE PROFILE:
{data['style']}

ORIGINAL NEWS ARTICLE:
{data['news']}

Instructions:
- Apply ALL patterns from the style profile
- Match their conversational voice markers and recurring phrases
- Use their sentence structures and paragraph rhythm
- Incorporate their authenticity markers (how they use specifics, admit uncertainty, etc)
- Follow their opening and closing patterns
- Avoid their anti-patterns
- Keep the same factual content but transform the voice completely

Output ONLY the rewritten article, no explanations."""
    
    restyle_start = time.time()
    logger.info("ğŸš€ Generating with model...")
    
    try:
        success,text,cost=generate(model,prompt,4000)
        
        restyle_duration = time.time() - restyle_start
        
        if not success:
            logger.error("âŒ GENERATION FAILED")
            logger.error(f"â±ï¸  Failed after: {restyle_duration:.2f}s")
            logger.error(f"ğŸ“› Error: {text}")
            logger.error("="*80)
            return jsonify({'error':text}),500
        
        logger.info("âœ… RESTYLE WITH MODEL COMPLETE")
        logger.info(f"â±ï¸  Duration: {restyle_duration:.2f}s")
        logger.info(f"ğŸ’° Cost: ${cost:.6f}")
        logger.info(f"ğŸ“ Output length: {len(text)} characters")
        logger.info("="*80)
        
        return jsonify({'styled':text,'cost':cost,'model':model})
    except Exception as e:
        restyle_duration = time.time() - restyle_start
        logger.error("âŒ RESTYLE ERROR")
        logger.error(f"â±ï¸  Failed after: {restyle_duration:.2f}s")
        logger.error(f"ğŸ“› Error: {e}",exc_info=True)
        logger.error("="*80)
        return jsonify({'error':str(e)}),500

@app.route('/available_models',methods=['GET'])
def available_models():
    logger.info("ğŸ“‹ /available_models called")
    models = get_models()
    logger.info(f"âœ… Returning {len(models)} models")
    return jsonify({'models':models})

@app.route('/chat',methods=['POST'])
def chat():
    data=request.json
    message=data.get('message','')
    model=data.get('model','claude-3-5-sonnet-latest')
    messages=data.get('messages',[{"role":"user","content":message}])
    temperature=float(data.get('temperature',1.0))
    max_tokens=int(data.get('max_tokens',1024))
    web_search=data.get('web_search',False)
    thinking=data.get('thinking',False)
    
    logger.info("="*80)
    logger.info("ğŸ’¬ CHAT REQUEST")
    logger.info(f"ğŸ“ Message: {message[:100]}{'...'if len(message)>100 else''}")
    logger.info(f"ğŸ¤– Model: {model}")
    logger.info(f"ğŸ” Web Search: {web_search}")
    logger.info(f"ğŸ§  Thinking: {thinking}")
    logger.info(f"ğŸŒ¡ï¸  Temperature: {temperature}")
    logger.info(f"ğŸ“Š Max Tokens: {max_tokens}")
    
    def generate():
        try:
            logger.info("ğŸš€ Initializing client...")
            start=time.time()
            tokens=0
            
            for text in stream_chat(model,messages,temperature,max_tokens,web_search,thinking):
                tokens+=1
                if tokens==1:
                    ttft = time.time()-start
                    logger.info(f"âš¡ First token in {ttft:.2f}s")
                yield f"data: {text}\n\n"
            
            elapsed=time.time()-start
            logger.info("âœ… CHAT COMPLETE")
            logger.info(f"ğŸ“Š Tokens: {tokens}")
            logger.info(f"â±ï¸  Duration: {elapsed:.2f}s")
            logger.info(f"âš¡ Speed: {tokens/elapsed:.1f} tok/s")
            yield"data: [DONE]\n\n"
            logger.info("="*80)
            
        except Exception as e:
            logger.error("âŒ CHAT ERROR")
            logger.error(f"ğŸ“› Error: {e}",exc_info=True)
            logger.error("="*80)
            yield f"data: Error: {str(e)}\n\n"
    
    return Response(generate(),mimetype='text/event-stream')

if __name__=='__main__':
    logger.info("="*80)
    logger.info("ğŸ§¹ Cleaning up port 5000...")
    kill_port_5000()
    time.sleep(0.5)
    
    logger.info("ğŸš€ STARTING FLASK APP")
    logger.info("ğŸ“± URL: http://127.0.0.1:5000")
    logger.info("âŒ¨ï¸  Press Ctrl+C to stop")
    logger.info("="*80)
    
    threading.Thread(target=lambda:time.sleep(1.5)or webbrowser.open('http://127.0.0.1:5000'),daemon=True).start()
    
    try:
        app.run(host='127.0.0.1',port=5000,debug=False,use_reloader=False)
    except KeyboardInterrupt:
        logger.info("\n")
        logger.info("="*80)
        logger.info("ğŸ‘‹ Shutting down gracefully...")
        logger.info("="*80)
        sys.exit(0)

================================================================================

FILE: ./utilities/google_secret_utils.py
--------------------------------------------------------------------------------
"""Google Secret Manager utilities for kumori apps"""
import subprocess
from google.cloud import secretmanager

def get_secret(secret_id: str, project_id: str = "kumori-404602"):
    """Get secret from GCP Secret Manager with auto project switching"""
    original = subprocess.run(['gcloud','config','get-value','project'], capture_output=True, text=True).stdout.strip()
    if original != project_id:
        subprocess.run(['gcloud','config','set','project',project_id], capture_output=True)
    
    try:
        client = secretmanager.SecretManagerServiceClient()
        name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
        secret = client.access_secret_version(request={"name": name}).payload.data.decode('UTF-8')
        if original != project_id:
            subprocess.run(['gcloud','config','set','project',original], capture_output=True)
        return secret
    except Exception as e:
        if original != project_id:
            subprocess.run(['gcloud','config','set','project',original], capture_output=True)
        return None

================================================================================

FILE: ./utilities/replicate_utils.py
--------------------------------------------------------------------------------
"""All Replicate functionality"""
import replicate
from .google_secret_utils import get_secret

MODELS = {
    "gpt-5-nano": {"id": "openai/gpt-5-nano", "name": "GPT-5 Nano", "description": "Fastest & cheapest", "pricing": {"input": 0.05, "output": 0.40}},
    "gpt-oss-120b": {"id": "openai/gpt-oss-120b", "name": "GPT-OSS 120B", "description": "Best value", "pricing": {"input": 0.18, "output": 0.72}},
    "gpt-4o-mini": {"id": "openai/gpt-4o-mini", "name": "GPT-4o Mini", "description": "Fast", "pricing": {"input": 0.15, "output": 0.60}},
    "gpt-5-mini": {"id": "openai/gpt-5-mini", "name": "GPT-5 Mini", "description": "Balanced", "pricing": {"input": 0.25, "output": 2.00}},
    "gemini-2.5-flash": {"id": "google/gemini-2.5-flash", "name": "Gemini 2.5 Flash", "description": "Google fast", "pricing": {"input": 2.50, "output": 2.50}},
    "gpt-5": {"id": "openai/gpt-5", "name": "GPT-5", "description": "Excellent", "pricing": {"input": 1.25, "output": 10.00}}
}

def get_client():
    return replicate.Client(api_token=get_secret('KUMORI_REPLICATE_API_KEY'))

def generate(model_key, prompt, max_tokens=2048):
    """Returns (success, text, cost)"""
    if model_key not in MODELS:
        return False, f"Unknown model: {model_key}", 0.0
    
    client = get_client()
    if not client:
        return False, "No client", 0.0
    
    try:
        output = client.run(MODELS[model_key]["id"], input={"prompt": prompt, "temperature": 0.7, "max_tokens": max_tokens})
        text = ''.join(str(x) for x in output) if hasattr(output, '__iter__') and not isinstance(output, str) else str(output)
        text = text.strip()
        
        pricing = MODELS[model_key]["pricing"]
        input_tokens = len(prompt) // 4
        output_tokens = len(text) // 4
        cost = (input_tokens / 1_000_000) * pricing["input"] + (output_tokens / 1_000_000) * pricing["output"]
        
        return True, text, cost
    except Exception as e:
        return False, str(e), 0.0

def get_models():
    return [{"key": k, "name": v["name"], "description": v["description"]} for k, v in MODELS.items()]

================================================================================

FILE: ./utilities/anthropic_utils.py
--------------------------------------------------------------------------------
"""All Anthropic/Claude functionality - optimized"""
import base64,concurrent.futures,re
from anthropic import Anthropic
from .google_secret_utils import get_secret

def get_client():
    return Anthropic(api_key=get_secret('KUMORI_ANTHROPIC_API_KEY'))

def strip_markdown_code_fence(text):
    """Remove markdown code fences (```json, ```, etc.) from text"""
    # Remove opening code fence (```json or ```)
    text = re.sub(r'^```(?:json)?\s*\n', '', text, flags=re.MULTILINE)
    # Remove closing code fence (```)
    text = re.sub(r'\n```\s*$', '', text, flags=re.MULTILINE)
    return text.strip()

def fetch_news_only():
    """Just fetch news - fast and cacheable"""
    client=get_client()
    r=client.messages.create(model="claude-sonnet-4-20250514",max_tokens=2000,
        tools=[{"type":"web_search_20250305","name":"web_search"}],
        messages=[{"role":"user","content":"Find the single biggest AI tech news story from today and summarize it in 3-4 paragraphs. Include what happened, why it matters, and key details."}])
    return ''.join(b.text for b in r.content if b.type=="text")

def analyze_style_only(files):
    """Just analyze style - returns JSON string"""
    client=get_client()
    content=[]
    
    for f in files:
        data=f.read()
        b64=base64.b64encode(data).decode('utf-8')
        ext=f.filename.split('.')[-1].lower()
        
        if ext=='pdf':
            content.append({"type":"document","source":{"type":"base64","media_type":"application/pdf","data":b64}})
        elif ext in['jpg','jpeg','png','gif','webp']:
            content.append({"type":"image","source":{"type":"base64","media_type":f"image/{ext if ext!='jpg'else'jpeg'}","data":b64}})
        else:
            text=data.decode('utf-8',errors='ignore')
            content.append({"type":"text","text":f"<document name='{f.filename}'>\n{text}\n</document>"})
    
    content.append({"type":"text","text":"""You are a writing style analyst. Analyze all provided documents to extract the author's complete writing DNA.

Extract and output a JSON object with this exact structure:

{
  "conversational_patterns": {
    "natural_voice_markers": ["phrases appearing 5+ times - their verbal tics and recurring language"],
    "opening_patterns": ["common first paragraph structures as templates with {placeholders}"],
    "transition_phrases": ["recurring connectors between ideas"],
    "closing_patterns": ["common ending structures as templates"]
  },
  
  "authenticity_markers": {
    "vulnerability_patterns": ["how they admit mistakes, share failures, show uncertainty"],
    "technical_authenticity": ["how they use real numbers, specs, metrics, timestamps, costs"],
    "what_they_never_fabricate": ["things they're always specific about vs vague"]
  },
  
  "style_fingerprints": {
    "sentence_patterns": ["typical structures - short? questions? compound?"],
    "paragraph_rhythm": "how they structure paragraphs",
    "tone": "their consistent voice quality",
    "technical_depth": "how deep they go with details"
  },
  
  "signature_elements": {
    "analogies": ["recurring metaphors or comparison styles"],
    "examples": ["types of examples they use"],
    "interaction_style": "how they address readers"
  },
  
  "anti_patterns": {
    "never_uses": ["words/phrases consistently absent"],
    "avoids": ["patterns they consciously avoid - hype, corporate speak, etc"]
  }
}

Rules:
- Only include patterns appearing in 3+ documents
- Use exact phrases from documents for voice_markers
- Create templates with {placeholders} for patterns
- Be specific, not generic
- Output ONLY valid JSON, no explanation"""})
    
    r=client.messages.create(model="claude-sonnet-4-20250514",max_tokens=8000,messages=[{"role":"user","content":content}])
    raw_text = r.content[0].text
    # Strip markdown code fences if present
    return strip_markdown_code_fence(raw_text)

def analyze_style_and_fetch_news(files):
    """Run style analysis and news fetch IN PARALLEL"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        style_future=executor.submit(analyze_style_only,files)
        news_future=executor.submit(fetch_news_only)
        
        style_json=style_future.result()
        news=news_future.result()
    
    return style_json,news

def restyle_content(style_json,news_text,model_name="Claude Sonnet 4"):
    """Restyle content using Claude"""
    client=get_client()
    
    prompt=f"""You are a writing style adapter. You have been given:

1. A detailed JSON style profile of a writer
2. An AI news article (originally written by {model_name})

Your task: Rewrite the news article to match the writer's style perfectly.

STYLE PROFILE:
{style_json}

ORIGINAL NEWS ARTICLE:
{news_text}

Instructions:
- Apply ALL patterns from the style profile
- Match their conversational voice markers and recurring phrases
- Use their sentence structures and paragraph rhythm
- Incorporate their authenticity markers (how they use specifics, admit uncertainty, etc)
- Follow their opening and closing patterns
- Avoid their anti-patterns
- Keep the same factual content but transform the voice completely

Output ONLY the rewritten article, no explanations."""
    
    r=client.messages.create(model="claude-sonnet-4-20250514",max_tokens=4000,messages=[{"role":"user","content":prompt}])
    return r.content[0].text

def stream_chat(model,messages,temperature,max_tokens,enable_web_search,enable_thinking):
    """Stream chat responses"""
    client=get_client()
    params={"model":model,"max_tokens":max_tokens,"temperature":temperature,"messages":messages}
    
    web_search_models=['claude-opus-4','claude-sonnet-4','claude-3-7-sonnet','claude-3-5-sonnet','claude-3-5-haiku']
    if enable_web_search and any(m in model for m in web_search_models):
        params["tools"]=[{"type":"web_search_20250305","name":"web_search"}]
    
    thinking_models=['claude-opus-4','claude-sonnet-4']
    if enable_thinking and any(m in model for m in thinking_models):
        params["thinking"]={"type":"enabled","budget_tokens":8000}
    
    with client.messages.stream(**params)as stream:
        for text in stream.text_stream:
            yield text

================================================================================

FILE: ./static/style.css
--------------------------------------------------------------------------------
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box
}

body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
    background: #0f0f0f;
    color: #e0e0e0;
    min-height: 100vh;
    display: flex;
    flex-direction: column
}

.nav {
    background: #1a1a1a;
    padding: 15px 20px;
    border-bottom: 1px solid #333;
    display: flex;
    align-items: center;
    gap: 20px
}

.nav-brand {
    font-size: 18px;
    font-weight: 600;
    color: #2196F3;
    text-decoration: none;
    margin-right: auto
}

.nav-links {
    display: flex;
    gap: 15px;
    list-style: none
}

.nav-link {
    color: #e0e0e0;
    text-decoration: none;
    padding: 8px 16px;
    border-radius: 6px;
    transition: background .2s;
    font-size: 14px
}

.nav-link:hover {
    background: #2a2a2a
}

.nav-link.active {
    background: #2196F3;
    color: #fff
}

.chat-wrap {
    display: flex;
    flex-direction: column;
    height: calc(100vh - 61px)
}

.header {
    background: #1a1a1a;
    padding: 15px 20px;
    border-bottom: 1px solid #333;
    display: flex;
    gap: 15px;
    flex-wrap: wrap;
    align-items: center
}

.header select,
.header input[type="range"] {
    background: #2a2a2a;
    color: #e0e0e0;
    border: 1px solid #444;
    padding: 6px 10px;
    border-radius: 6px;
    font-size: 13px
}

.toggle-group {
    display: flex;
    align-items: center;
    gap: 8px;
    font-size: 13px
}

.toggle {
    position: relative;
    display: inline-block;
    width: 42px;
    height: 22px
}

.toggle input {
    display: none
}

.slider {
    position: absolute;
    cursor: pointer;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-color: #444;
    transition: .3s;
    border-radius: 22px
}

.slider:before {
    position: absolute;
    content: "";
    height: 16px;
    width: 16px;
    left: 3px;
    bottom: 3px;
    background-color: #fff;
    transition: .3s;
    border-radius: 50%
}

input:checked+.slider {
    background-color: #2196F3
}

input:checked+.slider:before {
    transform: translateX(20px)
}

.chat-container {
    flex: 1;
    overflow-y: auto;
    padding: 20px
}

.message {
    margin-bottom: 16px;
    padding: 12px 16px;
    border-radius: 8px;
    max-width: 85%;
    line-height: 1.5
}

.user {
    background: #2a4a7c;
    margin-left: auto
}

.assistant {
    background: #2a2a2a;
    white-space: pre-wrap
}

.input-area {
    background: #1a1a1a;
    padding: 15px 20px;
    border-top: 1px solid #333;
    display: flex;
    gap: 10px
}

#messageInput {
    flex: 1;
    background: #2a2a2a;
    color: #e0e0e0;
    border: 1px solid #444;
    padding: 10px 15px;
    border-radius: 8px;
    font-size: 14px;
    resize: none;
    font-family: inherit
}

#sendBtn {
    background: #2196F3;
    color: #fff;
    border: none;
    padding: 10px 24px;
    border-radius: 8px;
    cursor: pointer;
    font-weight: 600;
    font-size: 14px
}

#sendBtn:hover {
    background: #1976D2
}

#sendBtn:disabled {
    background: #555;
    cursor: not-allowed
}

.slider-label {
    font-size: 12px;
    color: #999;
    display: flex;
    align-items: center;
    gap: 6px
}

.docs-wrap {
    padding: 40px 20px;
    max-width: 900px;
    margin: 0 auto
}

.upload-box {
    background: #1a1a1a;
    border: 2px dashed #444;
    border-radius: 12px;
    padding: 40px;
    text-align: center;
    margin-bottom: 30px
}

.upload-box h1 {
    margin-bottom: 10px;
    color: #2196F3;
    font-size: 28px
}

.upload-box p {
    color: #999;
    margin-bottom: 20px;
    font-size: 14px
}

input[type="file"] {
    margin: 20px 0;
    cursor: pointer
}

button {
    background: #2196F3;
    color: #fff;
    border: none;
    padding: 12px 32px;
    border-radius: 8px;
    cursor: pointer;
    font-size: 14px;
    font-weight: 600;
    transition: background .2s
}

button:hover {
    background: #1976D2
}

button:disabled {
    background: #555;
    cursor: not-allowed
}

.spinner {
    display: inline-block;
    width: 14px;
    height: 14px;
    border: 2px solid #fff;
    border-radius: 50%;
    border-top-color: transparent;
    animation: spin 1s linear infinite;
    margin-right: 8px
}

@keyframes spin {
    to {
        transform: rotate(360deg)
    }
}

.section {
    background: #2a2a2a;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
    display: none
}

.section-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    cursor: pointer;
    user-select: none;
    margin-bottom: 15px
}

.section-header h3 {
    color: #2196F3;
    font-size: 18px
}

.section-header .toggle-icon {
    color: #999;
    font-size: 20px;
    transition: transform .2s
}

.section-header .toggle-icon.open {
    transform: rotate(180deg)
}

.section-content {
    display: none;
    line-height: 1.6
}

.section-content.show {
    display: block
}

pre {
    background: #1a1a1a;
    padding: 15px;
    border-radius: 6px;
    overflow-x: auto;
    white-space: pre-wrap;
    font-size: 12px;
    line-height: 1.4;
    color: #0f0
}

.news-text {
    background: #1a1a1a;
    padding: 20px;
    border-radius: 6px;
    line-height: 1.8;
    font-size: 15px
}

.news-text p {
    margin-bottom: 12px
}

.styled-output {
    background: #1a1a1a;
    padding: 25px;
    border-radius: 6px;
    line-height: 1.8;
    font-size: 15px
}

.styled-output p {
    margin-bottom: 15px
}

.styled-output h1,
.styled-output h2,
.styled-output h3 {
    color: #2196F3;
    margin-top: 20px;
    margin-bottom: 10px
}

.cta-box {
    background: #1a4d2e;
    border: 1px solid #2a7a4a;
    border-radius: 8px;
    padding: 20px;
    text-align: center;
    margin-top: 15px
}

.cta-box h4 {
    color: #4ade80;
    margin-bottom: 10px;
    font-size: 16px
}

.cta-box button {
    background: #22c55e;
    font-size: 16px;
    padding: 14px 36px
}

.cta-box button:hover {
    background: #16a34a
}

.progress {
    color: #999;
    font-size: 13px;
    font-style: italic;
    margin-top: 10px
}

/* NEW: Model comparison styles */
.model-options {
    background: #1a1a1a;
    border: 1px solid #444;
    border-radius: 12px;
    padding: 30px;
    margin: 30px 0;
    display: none
}

.model-options h4 {
    margin: 0 0 8px 0;
    color: #2196F3;
    font-size: 20px
}

.model-options>p {
    margin: 0 0 20px 0;
    color: #999;
    font-size: 14px
}

.model-buttons {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
    gap: 12px;
    margin-top: 20px
}

.model-btn {
    display: flex;
    flex-direction: column;
    align-items: flex-start;
    width: 100%;
    padding: 16px 20px;
    background: rgba(59, 130, 246, 0.08);
    border: 1px solid rgba(59, 130, 246, 0.25);
    border-radius: 8px;
    color: #fff;
    cursor: pointer;
    transition: all 0.25s ease;
    text-align: left
}

.model-btn:hover {
    background: rgba(59, 130, 246, 0.15);
    border-color: rgba(59, 130, 246, 0.4);
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(59, 130, 246, 0.2)
}

.model-btn:active {
    transform: translateY(0)
}

.model-btn strong {
    display: block;
    margin-bottom: 6px;
    font-size: 15px;
    color: #60a5fa
}

.model-btn span {
    display: block;
    font-size: 12px;
    color: rgba(255, 255, 255, 0.5);
    font-weight: 400
}

.alt-models-container {
    margin-top: 30px;
    display: none
}

.model-output {
    background: #2a2a2a;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px
}

.model-output h3 {
    color: #2196F3;
    font-size: 16px;
    margin-bottom: 15px;
    display: flex;
    align-items: center;
    gap: 8px
}

.model-content {
    background: #1a1a1a;
    padding: 20px;
    border-radius: 6px;
    line-height: 1.8;
    font-size: 14px
}

.model-content p {
    margin-bottom: 12px
}

.cost-info {
    margin-top: 15px;
    padding: 10px 15px;
    background: rgba(34, 197, 94, 0.1);
    border: 1px solid rgba(34, 197, 94, 0.2);
    border-radius: 6px;
    font-size: 12px;
    color: #4ade80;
    display: inline-block
}

================================================================================

FILE: ./static/app.js
--------------------------------------------------------------------------------
let sty = null, nws = null;
let newsPromise = null; // Cache the news fetch promise
let newsFetchStartTime = null;

// Prefetch news immediately when file input is clicked
function prefetchNews() {
    if (newsPromise) {
        console.log('â„¹ï¸ News fetch already in progress, skipping duplicate request');
        return; // Already fetching
    }

    newsFetchStartTime = Date.now();
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('ğŸš€ NEWS FETCH STARTED IMMEDIATELY ON FILE BUTTON CLICK');
    console.log('â±ï¸  Started at:', new Date().toLocaleTimeString());
    console.log('ğŸ“¡ Endpoint: /prefetch_news');
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

    newsPromise = fetch('/prefetch_news')
        .then(r => {
            console.log('ğŸ“¥ Response received from server');
            console.log('â±ï¸  Response time:', (Date.now() - newsFetchStartTime) + 'ms');
            return r.json();
        })
        .then(d => {
            const totalTime = Date.now() - newsFetchStartTime;
            console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
            console.log('âœ… NEWS FETCH COMPLETE');
            console.log('â±ï¸  Total time:', totalTime + 'ms');
            console.log('ğŸ’¾ Cached:', d.cached || false);
            console.log('ğŸ“° News length:', d.news?.length || 0, 'characters');
            console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
            return d.news;
        })
        .catch(e => {
            const totalTime = Date.now() - newsFetchStartTime;
            console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
            console.error('âŒ NEWS FETCH ERROR');
            console.error('â±ï¸  Failed after:', totalTime + 'ms');
            console.error('ğŸ“› Error:', e);
            console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
            newsPromise = null; // Reset on error so it can retry
            throw e;
        });
}

// Get news (either from cache or wait for prefetch)
async function getNews() {
    if (!newsPromise) {
        console.log('âš ï¸  WARNING: No prefetch found, fetching now (this should not happen!)');
        prefetchNews();
    } else {
        console.log('â³ Waiting for prefetched news to complete...');
    }

    const result = await newsPromise;
    console.log('âœ… News ready for use!');
    return result;
}

function toggle(id) {
    let c = document.getElementById(id + '-content'),
        i = document.getElementById(id + '-icon');
    // Check computed style, not inline style
    let isHidden = window.getComputedStyle(c).display === 'none';
    if (isHidden) {
        c.style.display = 'block';
        i.textContent = 'â–¼';
    } else {
        c.style.display = 'none';
        i.textContent = 'â–¶';
    }
}

async function analyze() {
    let f = document.getElementById('files').files;
    if (!f.length) {
        alert('Please select files first!');
        return;
    }

    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('ğŸ“‚ FILE ANALYSIS STARTED');
    console.log('ğŸ“„ Number of files:', f.length);
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

    let s = document.getElementById('upload-status');
    s.innerHTML = '<p class="progress"><span class="spinner"></span>Analyzing your writing style...</p>';

    let fd = new FormData();
    for (let file of f) {
        console.log('ğŸ“ File:', file.name, '(' + (file.size / 1024).toFixed(1) + ' KB)');
        fd.append('files', file);
    }

    try {
        console.log('ğŸ“¤ Sending files to /analyze endpoint...');
        const analyzeStartTime = Date.now();

        let r = await fetch('/analyze', { method: 'POST', body: fd });

        console.log('ğŸ“¥ Analysis response received in', (Date.now() - analyzeStartTime) + 'ms');

        let d = await r.json();

        if (d.error) {
            console.error('âŒ Analysis error:', d.error);
            s.innerHTML = `<p style="color: #ef4444;">âŒ Error: ${d.error}</p>`;
            return;
        }

        sty = d.style_json;

        const totalAnalyzeTime = Date.now() - analyzeStartTime;
        console.log('âœ… Style analysis complete in', totalAnalyzeTime + 'ms');
        console.log('ğŸ“Š Style profile received');

        // Display JSON without code fence
        let styleText = typeof sty === 'string' ? sty : JSON.stringify(sty, null, 2);
        document.getElementById('style-json').textContent = styleText;
        document.getElementById('style-section').style.display = 'block';
        s.innerHTML = `<p style="color: #10b981;">âœ… Style analysis complete! (${(totalAnalyzeTime / 1000).toFixed(2)}s)</p>`;

        // Use prefetched news
        console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
        console.log('ğŸ“° NOW RETRIEVING PREFETCHED NEWS...');
        const newsWaitStart = Date.now();

        nws = await getNews();

        console.log('âœ… News retrieved in', (Date.now() - newsWaitStart) + 'ms');
        console.log('ğŸ“° News preview:', nws.substring(0, 100) + '...');
        console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

        let nt = document.getElementById('news-text');
        nt.innerHTML = nws.split('\n\n').map(p => `<p>${p}</p>`).join('');
        document.getElementById('news-section').style.display = 'block';
        document.getElementById('cta-box').style.display = 'block';

    } catch (e) {
        console.error('âŒ Analysis error:', e);
        s.innerHTML = `<p style="color: #ef4444;">âŒ Error: ${e.message}</p>`;
    }
}

async function restyle() {
    if (!sty || !nws) {
        alert('Please analyze files first!');
        return;
    }

    let btn = document.getElementById('restyle-btn');
    btn.disabled = true;
    btn.textContent = 'Generating...';

    let out = document.getElementById('styled-output');
    out.innerHTML = '<p class="progress"><span class="spinner"></span>Writing in your style...</p>';
    document.getElementById('styled-section').style.display = 'block';

    try {
        let claudeVersion = 'sonnet-4';
        let r = await fetch('/restyle', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ style: sty, news: nws, claude_version: claudeVersion })
        });

        let d = await r.json();

        if (d.error) {
            out.innerHTML = `<p style="color: #ef4444;">âŒ Error: ${d.error}</p>`;
            btn.disabled = false;
            btn.textContent = 'Write This in My Style';
            return;
        }

        let st = d.styled.split('\n\n').map(p => `<p>${p}</p>`).join('');
        out.innerHTML = st + `<div class="cost-info">ğŸ’° Cost: $${d.cost.toFixed(6)}</div>`;

        await showModelOptions();

        btn.disabled = false;
        btn.textContent = 'Write This in My Style';

    } catch (e) {
        out.innerHTML = `<p style="color: #ef4444;">âŒ Error: ${e.message}</p>`;
        btn.disabled = false;
        btn.textContent = 'Write This in My Style';
    }
}

async function showModelOptions() {
    try {
        let r = await fetch('/available_models');
        let data = await r.json();

        let html = `
            <h4>âœ¨ Want to see how other AI models would write this?</h4>
            <p>Compare different models' interpretations of your style:</p>
            <div class="model-buttons">
        `;

        for (let model of data.models) {
            html += `
                <button class="model-btn" onclick="restyleWithModel('${model.key}')">
                    <strong>${model.name}</strong>
                    <span>${model.description}</span>
                </button>
            `;
        }

        html += '</div>';

        document.getElementById('model-options').innerHTML = html;
        document.getElementById('model-options').style.display = 'block';

    } catch (e) {
        console.error('Error loading models:', e);
    }
}

async function restyleWithModel(modelKey) {
    let container = document.getElementById('alt-models-output');
    container.style.display = 'block';

    let modelDiv = document.getElementById(`model-${modelKey}`);
    if (!modelDiv) {
        modelDiv = document.createElement('div');
        modelDiv.id = `model-${modelKey}`;
        modelDiv.className = 'model-output';
        container.appendChild(modelDiv);
    }

    modelDiv.innerHTML = `
        <h3>ğŸ¤– ${modelKey}</h3>
        <div class="model-content">
            <p class="progress"><span class="spinner"></span>Generating with ${modelKey}...</p>
        </div>
    `;

    modelDiv.scrollIntoView({ behavior: 'smooth', block: 'start' });

    let cleanSty = typeof sty === 'string' ? sty : JSON.stringify(sty);

    try {
        let r = await fetch('/restyle_with_model', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                style: cleanSty,
                news: nws,
                model: modelKey,
                claude_version: modelKey
            })
        });

        let d = await r.json();

        if (d.error) {
            modelDiv.querySelector('.model-content').innerHTML = `
                <p style="color: #ef4444;">âŒ Error: ${d.error}</p>
            `;
            return;
        }

        let st = d.styled.split('\n\n').map(p => `<p>${p}</p>`).join('');
        modelDiv.querySelector('.model-content').innerHTML = st +
            `<div class="cost-info">ğŸ’° Cost: $${d.cost.toFixed(6)}</div>`;

    } catch (e) {
        modelDiv.querySelector('.model-content').innerHTML = `
            <p style="color: #ef4444;">âŒ Error: ${e.message}</p>
        `;
    }
}

let chatMessages = [];

function updateSlider(id) {
    document.getElementById(id + 'Value').textContent = document.getElementById(id + 'Slider').value;
}

document.getElementById('tempSlider')?.addEventListener('input', () => updateSlider('temp'));
document.getElementById('tokenSlider')?.addEventListener('input', () => updateSlider('token'));

async function send() {
    let input = document.getElementById('messageInput'),
        msg = input.value.trim();
    if (!msg) return;

    input.value = '';

    let container = document.getElementById('chatContainer'),
        userDiv = document.createElement('div');
    userDiv.className = 'message user';
    userDiv.textContent = msg;
    container.appendChild(userDiv);

    chatMessages.push({ role: 'user', content: msg });

    let assistantDiv = document.createElement('div');
    assistantDiv.className = 'message assistant';
    assistantDiv.textContent = '';
    container.appendChild(assistantDiv);

    container.scrollTop = container.scrollHeight;

    try {
        let r = await fetch('/chat', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model: document.getElementById('modelSelect').value,
                messages: chatMessages,
                temperature: parseFloat(document.getElementById('tempSlider').value),
                max_tokens: parseInt(document.getElementById('tokenSlider').value),
                web_search: document.getElementById('webSearchToggle').checked,
                thinking: document.getElementById('thinkingToggle').checked
            })
        });

        let reader = r.body.getReader(),
            decoder = new TextDecoder(),
            fullText = '';

        while (true) {
            let { done, value } = await reader.read();
            if (done) break;

            let chunk = decoder.decode(value),
                lines = chunk.split('\n');

            for (let line of lines) {
                if (line.startsWith('data: ')) {
                    let data = line.slice(6);
                    if (data === '[DONE]') break;
                    if (data.startsWith('Error:')) {
                        assistantDiv.textContent += '\n\n' + data;
                        break;
                    }
                    fullText += data;
                    assistantDiv.textContent = fullText;
                    container.scrollTop = container.scrollHeight;
                }
            }
        }

        chatMessages.push({ role: 'assistant', content: fullText });

    } catch (e) {
        assistantDiv.textContent = 'Error: ' + e.message;
    }
}

================================================================================

FILE: ./static/stories/story1.txt
--------------------------------------------------------------------------------

Anthropicâ€™s billion-dollar TPU expansion signals a strategic shift in enterprise AI infrastructure

Anthropicâ€™s announcement this week that it will deploy up to one million Google Cloud TPUs in a deal worth tens of billions of dollars marks a significant recalibration in enterprise AI infrastructure strategy. 

The expansion, expected to bring over a gigawatt of capacity online in 2026, represents one of the largest single commitments to specialised AI accelerators by any foundation model providerâ€”and offers enterprise leaders critical insights into the evolving economics and architecture decisions shaping production AI deployments.

The move is particularly notable for its timing and scale. Anthropic now serves more than 300,000 business customers, with large accountsâ€”defined as those representing over US$100,000 in annual run-rate revenueâ€”growing nearly sevenfold in the past year. 

This customer growth trajectory, concentrated among Fortune 500 companies and AI-native startups, suggests that Claudeâ€™s adoption in enterprise environments is accelerating beyond early experimentation phases into production-grade implementations where infrastructure reliability, cost management, and performance consistency become non-negotiable.

The multi-cloud calculus
What distinguishes this announcement from typical vendor partnerships is Anthropicâ€™s explicit articulation of a diversified compute strategy. The company operates across three distinct chip platforms: Googleâ€™s TPUs, Amazonâ€™s Trainium, and NVIDIAâ€™s GPUs. 

CFO Krishna Rao emphasised that Amazon remains the primary training partner and cloud provider, with ongoing work on Project Rainierâ€”a massive compute cluster spanning hundreds of thousands of AI chips across multiple US data centres.

For enterprise technology leaders evaluating their own AI infrastructure roadmaps, this multi-platform approach warrants attention. It reflects a pragmatic recognition that no single accelerator architecture or cloud ecosystem optimally serves all workloads. 

Training large language models, fine-tuning for domain-specific applications, serving inference at scale, and conducting alignment research each present different computational profiles, cost structures, and latency requirements.

The strategic implication for CTOs and CIOs is clear: vendor lock-in at the infrastructure layer carries increasing risk as AI workloads mature. Organisations building long-term AI capabilities should evaluate how model providersâ€™ own architectural choicesâ€”and their ability to port workloads across platformsâ€”translate into flexibility, pricing leverage, and continuity assurance for enterprise customers.

Price-performance and the economics of scale
Google Cloud CEO Thomas Kurian attributed Anthropicâ€™s expanded TPU commitment to â€œstrong price-performance and efficiencyâ€ demonstrated over several years. While specific benchmark comparisons remain proprietary, the economics underlying this choice matter significantly for enterprise AI budgeting.

TPUs, purpose-built for tensor operations central to neural network computation, typically offer advantages in throughput and energy efficiency for specific model architectures compared to general-purpose GPUs. The announcementâ€™s reference to â€œover a gigawatt of capacityâ€ is instructive: power consumption and cooling infrastructure increasingly constrain AI deployment at scale. 

For enterprises operating on-premises AI infrastructure or negotiating colocation agreements, understanding the total cost of ownershipâ€”including facilities, power, and operational overheadâ€”becomes as critical as raw compute pricing.

The seventh-generation TPU, codenamed Ironwood and referenced in the announcement, represents Googleâ€™s latest iteration in AI accelerator design. While technical specifications remain limited in public documentation, the maturity of Googleâ€™s AI accelerator portfolioâ€”developed over nearly a decadeâ€”provides a counterpoint to enterprises evaluating newer entrants in the AI chip market. 

Proven production history, extensive tooling integration, and supply chain stability carry weight in enterprise procurement decisions where continuity risk can derail multi-year AI initiatives.

Implications for enterprise AI strategy
Several strategic considerations emerge from Anthropicâ€™s infrastructure expansion for enterprise leaders planning their own AI investments:

Capacity planning and vendor relationships: The scale of this commitmentâ€”tens of billions of dollarsâ€”illustrates the capital intensity required to serve enterprise AI demand at production scale. Organisations relying on foundation model APIs should assess their providersâ€™ capacity roadmaps and diversification strategies to mitigate service availability risks during demand spikes or geopolitical supply chain disruptions.

Alignment and safety testing at scale: Anthropic explicitly connects this expanded infrastructure to â€œmore thorough testing, alignment research, and responsible deployment.â€ For enterprises in regulated industriesâ€”financial services, healthcare, government contractingâ€”the computational resources dedicated to safety and alignment directly impact model reliability and compliance posture. Procurement conversations should address not just model performance metrics, but the testing and validation infrastructure supporting responsible deployment.

Integration with enterprise AI ecosystems: While this announcement focuses on Google Cloud infrastructure, enterprise AI implementations increasingly span multiple platforms. Organisations using AWS Bedrock, Azure AI Foundry, or other model orchestration layers must understand how foundation model providersâ€™ infrastructure choicesaffect API performance, regional availability, and compliance certifications across different cloud environments.

The competitive landscape: Anthropicâ€™s aggressive infrastructure expansion occurs against intensifying competition from OpenAI, Meta, and other well-capitalised model providers. For enterprise buyers, this capital deployment race translates into continuous model capability improvementsâ€”but also potential pricing pressure, vendor consolidation, and shifting partnership dynamics that require active vendor management strategies.

The broader context for this announcement includes growing enterprise scrutiny of AI infrastructure costs. As organisations move from pilot projects to production deployments, infrastructure efficiency directly impacts AI ROI. 

Anthropicâ€™s choice to diversify across TPUs, Trainium, and GPUsâ€”rather than standardising on a single platformâ€”suggests that no dominant architecture has emerged for all enterprise AI workloads. Technology leaders should resist premature standardisation and maintain architectural optionality as the market continues to evolve rapidly.

================================================================================

FILE: ./static/stories/story2.txt
--------------------------------------------------------------------------------
OpenAI connects ChatGPT to enterprise data to surface knowledge

OpenAI is surfacing company knowledge by connecting ChatGPT to enterprise data, turning it from a general assistant into a custom analyst.

For business leaders, generative AIâ€™s potential has always been limited by its lack of access to internal data. Even the best AI isnâ€™t helpful if it canâ€™t access the info needed to do a job. OpenAI points out that the info you need is often in your internal tools, but that knowledge is scattered across documents, files, messages, emails, tickets, and project trackers.

This scattering is more than just annoying; it hurts efficiency and decisionmaking. The main problem is that these tools donâ€™t always connect, and the best answer is often spread across all of them.

This puts OpenAI up against the AI strategies of big enterprise platforms like Microsoftâ€™s Copilot in Azure and Office 365, Googleâ€™s Vertex AI, Salesforceâ€™s Agentforce, and AWS Bedrock. Everyone is racing to connect models to secure company data.

OpenAI uses third-party data for ChatGPT enterprise tasks
ChatGPT will connect to apps like Slack, SharePoint, Google Drive, and GitHub. OpenAI says itâ€™s powered by a version of GPT-5, trained to check many sources for better answers. For checking and validation, every answer shows where the info came from.

OpenAI ChatGPT AI virtual assistant connecting to enterprise apps to surface company knowledge.
This changes what you can do from simple writing to complex analysis. For example, a manager prepping for a client call could ask for a briefing. The model could then use recent Slack messages, email details, call notes from Google Docs, and support tickets from Intercom to make a summary.

This power can also handle confusion. If you ask, â€œWhat are the company goals for next year?â€, the tool will summarise whatâ€™s been talked about and point out different opinions. This goes beyond just finding data; now itâ€™s analysing situations and helping leaders find disagreements or unfinished decisions.

Other uses for teams:

Strategy: Putting together customer feedback from Slack, survey results from Google Slides, and main topics from support tickets to plan roadmaps.
Reporting: Making campaign summaries by getting data from HubSpot, briefs from Google Docs, and key points from emails.
Planning: Helping engineering leads plan releases by checking GitHub for open tasks, checking Linear for tickets, and checking Slack for bug reports.
Addressing enterprise AI governance and implementation
For CISOs and data leaders, sharing intellectual property with an AI model is a big risk. OpenAI is dealing with this by focusing on admin controls and data privacy.

The most important control is that the system respects your current company permissions. OpenAI has ensured that ChatGPT can only see the enterprise data that each user can already see.

ChatGPT Enterprise and Edu admins can manage access to apps and create custom roles. OpenAI says it doesnâ€™t train on your data by default. It also has security features like encryption, SSO, SCIM, IP whitelisting, and a Compliance API for logs.

But, tech leaders should know the limits. Itâ€™s not perfect yet. Users have to pick it when starting a conversation. Also, thereâ€™s a trade-off: when company knowledge is on, ChatGPT canâ€™t search the web or make charts. OpenAI is working to fix this soon.

The toolâ€™s usefulness depends on its ecosystem. Itâ€™s launching with key platforms and adding connectors for tools like Asana, GitLab Issues, and ClickUp, copying the strategies of IBM watsonx and SAP Joule.

OpenAIâ€™s enterprise data knowledge surfacing is the next step for AI assistants like ChatGPT, moving them into the private core of businesses. It tries to solve the AI problem: connecting models to the data where work happens.

For business leaders, this means:

Check your data: Before using this, CISOs and CDAOs must check that data permissions in SharePoint, Google Drive, etc., are correct. The AI will only respect those permissions, so if theyâ€™re too open, the AI will show that weakness.
Pilot with tricky tasks: Instead of rolling it out to everyone, find specific workflows that are slowed down by scattered info. Preparing client briefings or making cross-department reports are good places to start measuring results.
Set expectations: Teams must know the limits. Having to manually turn it on and not being able to search the web at the same time are big limits to consider.
Watch the ecosystem: The toolâ€™s value will depend on its integrations. CIOs should compare the toolâ€™s connector list to their companyâ€™s tech.
Compare to current platforms: See how this compares to the AI solutions from Microsoft, Google, and Salesforce. The decision is quickly becoming about which data ecosystem offers the most secure, integrated, and cost-effective path.
OpenAIâ€™s new company knowledge feature shows that the most important thing for generative AI is now secure and useful data integration, not just how good the model is.

This latest ChatGPT feature should make things much faster by getting rid of enterprise knowledge silos, but it also makes data governance and access control more important than ever. For business leaders, this tech isnâ€™t a simple fix. Instead, itâ€™s a good reason to get their data organised before others do.

================================================================================

FILE: ./static/stories/story3.txt
--------------------------------------------------------------------------------

Autonomy in the real world? Druid AI unveils AI agent â€˜factoryâ€™

At its London Symbiosis 4 event on 22 October, Druid AI introduced what it terms Virtual Authoring Teams â€“ a new generation of AI agents that can design, test, and deploy other AI agents. The announcement marks a move towards what the company calls a â€˜factory modelâ€™ for AI automation.

According to Druid, the system enables organisations to build enterprise-grade AI agents up to ten times faster, and the platform offers orchestration facilities, plus compliance safeguards and measurable ROI tracking. The orchestration engine, Druid Conductor, serves as a control layer that integrates data, tooling, and human oversight into a single framework.

In addition to the Druid Conductor is the Druid Agentic Marketplace, a repository of pre-built, industry-specific agents for banking, healthcare, education, and insurance. With its solutions, Druid wants to make agentic AI accessible to non-technical users, but provide scalability capability suitable for enterprise use.

Chief Executive Joe Kim described it as â€œAI [that] actually worksâ€ â€“ a bold claim in a market flooded with experimentation and unproven automation frameworks.

The new agentic battleground
Druid is not alone in its pursuit. Similar platforms, the likes of Cognigy, Kore.ai, and Amelia, each represent heavy investment in multi-agent orchestration environments. OpenAIâ€™s GPTs and Anthropicâ€™s Claude Projects also allow users to design semi-autonomous digital workers without coding expertise.

Googleâ€™s Vertex AI Agents and Microsoftâ€™s Copilot Studio are moving in the same direction, placing agentic AI as an extension to enterprise ecosystems rather than stand-alone products.

The difference between the competing platforms lies in execution â€“ some focus on workflow automation, others on conversational depth or ease of integration with other parts of the IT stack.

For technology buyers, such diversity is an opportunity and a risk. Vendors are racing to define what agentic AI means in practice, and thereâ€™s an undoubted element of agentic AI being 2025â€™s buzzword, implying differentiation between pure LLM models and practical tools useful in business contexts. Some vendors view agentic as an architecture â€“ modular, distributed, and explainable, while others frame agentic AI as a layer of automation that builds itself â€“ or rather, can discover what powers itâ€™s been granted, and use them according to natural language instructions. The truth of agentic AIâ€™s abilities sits somewhere between engineering promises and operational reality.

The business case â€“ and the caveats
Agentic AI systems promise extraordinary benefits. They can accelerate routine development, coordinate multiple business functions, and use data repositories that were once siloed. For enterprises under pressure to deliver digital transformation with limited headcount, the idea of self-building AI teams is compelling.

But the use of the conditional tense in many vendorsâ€™ marketing materials and descriptions is telling: agentic AI can achieve savings, could drive faster operations, and so on.

Business leaders should approach such systems with a clear head. There are few proven case studies beyond pilot programmes inside large corporations (those with mature data governance and deep budgets), and even in those organisations, the returns have been uneven. Failures are rarely shouted from the rooftops, after all.

The biggest risks are not technical â€“ theyâ€™re organisational. Delegating complex decision-making to automated agents without sufficient oversight introduces potential bias, compliance breaches, and reputational exposure. Systems can also generate automation debt: a growing tangle of interconnected bots that become difficult to monitor or update as business processes evolve.

The issue of necessary organisational change is troubling on two counts, furthermore. Most business processes have evolved a particular way for good reasons, so why change them to implement a new, largely unproven technology? Secondly, whatâ€™s often proposed is change thatâ€™s instigated by technology implementation. Shouldnâ€™t processes change for strategic reasons, and technology support that change? Is this a case of the IT tail wagging the business dog?

Security remains a further concern. Each agent increases the surface area for potential breaches or data misuse, particularly when they are designed to communicate and collaborate autonomously. As more workflows become self-directed, ensuring traceability and accountability becomes essential, and more difficult to unpick as complexity increases. The necessary headcount to monitor results and ensure rigorous oversight could negate any ROI agentic AI offers.

Why agentic AI attracts enterprises
Despite the challenges, the attraction is easy to understand. A successful agentic system can transform the speed at which an enterprise experiments and scales. By delegating repeatable cognitive tasks â€“ from compliance checks to customer service triage â€“ organisations can redirect human activity elsewhere.

Druidâ€™s Virtual Authoring Teams encapsulate the logic: automate the automation. Its marketplace of domain-specific agents offers enterprises a head start, promising faster deployments and measurable ROI. For sectors struggling with talent shortages and regulatory pressure, that is an appealing prospect.

Moreover, Druidâ€™s emphasis on explainable AI and its orchestration layer suggests an awareness of corporate caution. Its stated pillars â€“ control, accuracy, and results â€“ are designed to reassure boards that transparency can coexist with speed. If the system truly delivers what the company claims, it could narrow the gap between AI experimentation and scalable transformation.

Balancing autonomy with accountability
Still, for every organisation embracing agentic AI, another remains unconvinced. Many enterprises are wary of over-promising vendors and pilot fatigue. A technology capable of designing and deploying its own successors raises operational questions. What happens when an agent acts beyond its creatorâ€™s intent? How do governance frameworks keep pace?

Business leaders must treat autonomy as a spectrum, not a goal. The near future of enterprise AI will likely blend human-supervised automation with limited agentic autonomy. Systems like Druidâ€™s may act as orchestration hubs rather than fully independent actors.

From hype to utility
Agentic AI represents a natural evolution of automation in a wild frontier. Its potential is obvious, yet the market still lacks broad, evidence-based validation of sustained business outcomes. It may just be early days, or may be hyperbole drowning out the voices of reason.

For now, agentic systems do work in controlled contexts â€“ contact-centre operations, document processing, and IT service management. Scaling agentic AI across organisations will require maturity not just in technology, but in culture, process design, and methods of oversight.

As Druid and its peers expand their offerings, enterprises will need to weigh the cost of control against the promised wins from better automation. The next two years will determine whether AI factories become a part of business operations, or another layer of abstraction with its own overheads.

================================================================================

FILE: ./templates/base.html
--------------------------------------------------------------------------------
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <title>{% block title %}Style Analyzer{% endblock %}</title>
    <link rel="stylesheet" href="{{ url_for('static',filename='style.css') }}">
</head>

<body>
    <nav class="nav">
        <a href="{{ url_for('home') }}" class="nav-brand">âœï¸ Writing Style Analyzer</a>
    </nav>
    {% block content %}{% endblock %}
    <script src="{{ url_for('static',filename='app.js') }}"></script>
</body>

</html>

================================================================================

FILE: ./templates/style.html
--------------------------------------------------------------------------------
{% extends "base.html" %}
{% block title %}Style Analyzer{% endblock %}
{% block content %}
<div class="docs-wrap">
    <div class="upload-box" id="upload-box">
        <h1>âœï¸ Discover Your Writing Voice</h1>
        <p>Upload 2-5 writing samples to analyze your unique style, then see AI news written in YOUR voice</p>
        <input type="file" id="files" multiple accept=".txt,.md,.pdf,.jpg,.jpeg,.png,.doc,.docx"
            onclick="prefetchNews()" onchange="analyze()">
        <div id="upload-status"></div>
    </div>

    <div class="section" id="style-section">
        <div class="section-header" onclick="toggle('style')">
            <h3>ğŸ“Š Your Style Profile (JSON)</h3>
            <span class="toggle-icon" id="style-icon">â–¶</span>
        </div>
        <div class="section-content" id="style-content">
            <pre id="style-json"></pre>
        </div>
    </div>

    <div class="section" id="news-section">
        <div class="section-header" onclick="toggle('news')">
            <h3>ğŸ“° Latest AI News (Original)</h3>
            <span class="toggle-icon" id="news-icon">â–¶</span>
        </div>
        <div class="section-content" id="news-content">
            <div class="news-text" id="news-text"></div>
        </div>
    </div>

    <div class="cta-box" id="cta-box" style="display:none;">
        <h4>âœ¨ Ready to see this story in YOUR voice?</h4>
        <button id="restyle-btn" onclick="restyle()">Write This in My Style</button>
    </div>

    <div class="section" id="styled-section">
        <h3>âœï¸ Written in Your Style (Claude Sonnet 4)</h3>
        <div class="styled-output" id="styled-output"></div>
    </div>

    <div class="model-options" id="model-options"></div>

    <div class="alt-models-container" id="alt-models-output"></div>
</div>
{% endblock %}

================================================================================

FILE: ./templates/chat.html
--------------------------------------------------------------------------------
{% extends "base.html" %}
{% block title %}Chat{% endblock %}
{% block content %}
<div class="chat-wrap">
    <div class="header">
        <select id="modelSelect">{% for n,v in models.items() %}<option value="{{v}}">{{n}}</option>{% endfor
            %}</select>
        <div class="toggle-group"><label class="toggle"><input type="checkbox" id="webSearchToggle"><span
                    class="slider"></span></label><span>Web Search</span></div>
        <div class="toggle-group"><label class="toggle"><input type="checkbox" id="thinkingToggle"><span
                    class="slider"></span></label><span>Thinking</span></div>
        <div class="slider-label">Temp: <span id="tempValue">1.0</span><input type="range" id="tempSlider" min="0"
                max="1" step="0.1" value="1.0" style="width:80px;"></div>
        <div class="slider-label">Tokens: <span id="tokenValue">1024</span><input type="range" id="tokenSlider"
                min="256" max="4096" step="256" value="1024" style="width:100px;"></div>
    </div>
    <div class="chat-container" id="chatContainer"></div>
    <div class="input-area">
        <textarea id="messageInput" rows="2" placeholder="Type your message..."
            onkeydown="if(event.key==='Enter'&&!event.shiftKey){event.preventDefault();send();}"></textarea>
        <button id="sendBtn" onclick="send()">Send</button>
    </div>
</div>
{% endblock %}

================================================================================

